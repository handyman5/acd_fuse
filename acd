#!/usr/bin/env python2

from copy import deepcopy
from errno import *
from glob import glob
from stat import S_IFDIR, S_IFREG
from sys import exit
from time import time
import fuse
import hashlib
import json
import logging
import os
import os.path
import pickle
import tempfile

_THIS_DIR = os.path.dirname(os.path.realpath(__file__))

try:
    import pyacd
except ImportError:
    from sys import path
    path.append(os.path.join(_THIS_DIR, 'pyacd'))
    import pyacd

fuse.fuse_python_api = (0, 2)

myname = "acd_fuse"

folder_types = ["ROOT", "FOLDER", "RECYCLE"]

log = logging.getLogger(myname)

class ACDFS(fuse.Fuse):
    api = None
    cache = None
    cachefree = 10
    download = None
    email = None
    log = None
    password = None
    session = None
    sessionfile = None
    tempdir = None

    dircache = {}
    infocache = {}

    def __init__(self, session=None, *args, **kw):
        '''Initializes an Amazon Cloud Drive FUSE filesystem.

        :param session: a Session object to create the FS with (optional)
        :type session: Session
        '''
        fuse.Fuse.__init__(self, *args, **kw)
        self.api = pyacd.api
        self.tempdir = os.path.join(tempfile.gettempdir(), myname)
        if not os.path.exists(self.tempdir): os.mkdir(self.tempdir)
        self.sessionfile = os.path.join(self.tempdir, "sessionfile")

        # setup logging
        self.log = logging.getLogger(myname)
        hdlr = logging.FileHandler(os.path.join(self.tempdir, 'debug.log'))
        self.log.addHandler(hdlr)
        self.log.setLevel(logging.DEBUG)

    def main(self):
        try:
            print "Trying to login from cached sessionfile %s" % self.sessionfile
            with open(self.sessionfile, "rb") as sessfile:
                session = pyacd.auth.Session()
                session.__dict__ = pickle.load(sessfile)
                self.session = pyacd.login(session=session)
                self.session.__dict__['agreed_with_terms']=True
                self.session.username = self.email
            if not self.session.is_logged_in():
                raise ValueError
        except (AttributeError, IOError, ValueError, pyacd.exception.PyAmazonCloudDriveError):
            print "Cached session failed; trying auth login"
            if not self.email or not self.password:
                # if no email or password and they're needed, error out and die
                raise ValueError("No login information provided; try again with mount options 'email' and 'password'")
            with open(self.sessionfile, "wb") as sessfile:
                self.session = pyacd.login(self.email, self.password)
                self.session.__dict__['agreed_with_terms']=True
                self.session.username = self.email
                pickle.dump(self.session.__dict__, sessfile)
        print "Login successful; starting filesystem"
        self.download = self.api.can_device_download()

        fuse.Fuse.main(self)

    def getinfo(self, path):
        '''Retrieve an Info object for the given path, either from cache or from ACD.

        :param path: Full path and filename in ACD
        :type path: str
        :returns: an Info object representing the path
        '''
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"getinfo: path %s" % path)
        if path not in self.infocache:
            self.log.debug(u"  lookup failed for path %s" % path)
            info = self.api.get_info_by_path(path.encode('utf_8'))
            self.infocache[path] = info
        return self.infocache[path]

    #####
    # Local filecache
    #####
    def cache_path(self, path):
        '''Resolve the ACD path into a filename in local cache

        :param path: Full path and filename in ACD
        :type path: str
        :returns: the local path to the cached file
        '''
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"cache_path: path %s" % path)
        info = self.getinfo(path)

        return os.path.join(self.tempdir, info.object_id)

    def check_filecache(self, path):
        '''Determine whether an identical file exists in the filecache

        :param path: Full path and filename in ACD
        :type path: str
        :returns: `bool`
        '''
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"check_filecache: path %s" % path)
        info = self.getinfo(path)

        try:
            dpath = self.cache_path(path)
            r_size = info.size
            l_size = os.stat(dpath)[6]
            if r_size == l_size:
                with open(dpath, 'r') as dfile:
                    r_md5 = info.md5
                    l_md5 = hashlib.md5(dfile.read()).hexdigest()
                    return (r_md5 == l_md5)
        except OSError:
            # file does not exist in cache
            return False

    def read_filecache(self, path):
        '''Reads a file from the filecache (basically translating an ACD path into bytes)

        :param path: Full path and filename in ACD
        :type path: str
        :returns: file data
        '''
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"read_filecache: path %s" % path)
        info = self.getinfo(path)

        with open(self.cache_path(path), 'rb') as dfile:
            self.log.debug(u"  Reading data from %s" % self.cache_path(path))
            return dfile.read()

    def write_filecache(self, path, data):
        '''Writes a file into the filecache in preparation for uploading to ACD

        :param path: Full path and filename in ACD
        :type path: str
        '''
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"write_filecache: path %s" % path)
        info = self.getinfo(path)

        if self.check_filecache(path):
            self.log.info("  Skipping write because file already cached")
            return

        with open(self.cache_path(path), 'ab') as dfile:
            self.log.debug(u"  Writing data to %s" % self.cache_path(path))
            dfile.write(data)

    def clear_filecache(self, path):
        '''Removes a file from the filecache (used when it's deleted in ACD)

        :param path: Full path and filename in ACD
        :type path: str
        '''
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"clear_filecache: path %s" % path)
        info = self.getinfo(path)

        dpath = self.cache_path(path)
        if os.path.exists(dpath):
            self.log.info("  Clearing cache %s" % dpath)
            os.unlink(dpath)

    def gc_filecache(self):
        '''Runs a garbage collection on the filecache

        In order to prevent the filecache from growing without bound,
        this function will remove the least recently used files until the
        disk the filecache is on is below the fullness threshold.
        '''
        while True:
            disk = os.statvfs(self.tempdir)
            available = float(disk.f_bsize * disk.f_bavail)
            capacity = float(disk.f_bsize * disk.f_blocks)
            percent_free = (available / capacity) * 100

            self.log.info("gc_filecache: percent_free: %f; self.cachefree: %f" % (percent_free, self.cachefree))

            if percent_free > self.cachefree:
                break

            files = glob(self.tempdir + "/*-*-*-*-*")
            if not files:
                break

            files.sort(key=lambda x: os.path.getmtime(x))
            victim = files[-1]
            self.log.info("  removing file %s" % victim)
            os.unlink(victim)

    #####
    # Standard FUSE methods
    #####
    def statfs(self):
        blocksize = 1
        storage = self.api.get_user_storage()
        total = storage.total_space
        free = storage.free_space

        myfs = fuse.StatVfs()
        myfs.f_bsize = blocksize
        myfs.f_frsize = blocksize
        myfs.f_blocks = total
        myfs.f_bfree = free
        myfs.f_bavail = free
        return myfs

    def getattr(self, path):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"getattr: path %s" % path)

        head, tail = os.path.split(path)
        nopaths = [u'.git', u'objects']
        if tail in nopaths:
            return -ENOENT

        try:
            info = self.getinfo(path)
        except pyacd.PyAmazonCloudDriveError as e:
            print e
            return -ENOENT

        st = fuse.Stat()
        st.st_atime = int(time())
        st.st_mtime = info.modified
        st.st_ctime = info.created

        if info.Type in folder_types:
            st.st_mode = S_IFDIR | 0755
            st.st_nlink = 2
            return st
        elif info.Type == "FILE":
            st.st_mode = S_IFREG | 0644
            st.st_nlink = 1
            st.st_size = info.size and info.size or 0
            return st
        else: return -ENOENT

    def readdir(self, path, offset):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"readdir: path %s, offset %s" % (path, offset))

        yield fuse.Direntry('.')
        yield fuse.Direntry('..')

        if path in self.dircache:
            for x in self.dircache[path]:
                yield fuse.Direntry(x)
        else:
            info = self.getinfo(path)
            if info.Type not in folder_types:
                raise StopIteration

            folder = self.api.list_by_id(info.object_id)
            names = [x.name.encode('utf_8', 'replace') for x in folder.objects]
            self.dircache[path] = names

            for x in names:
                yield fuse.Direntry(x)

    def read(self, path, size, offset):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"read: path %s, size %d, offset %d" % (path, size, offset))

        if not self.download:
            return -EPERM

        data = None
        info = self.getinfo(path)
        try:
            self.log.info("Trying to read data from filecache for %s" % path)
            data = self.read_filecache(path)
        except IOError:
            self.log.info("Filecache failed; downloading")
            data = self.api.download_by_id(info.object_id)
            self.write_filecache(path, data)

        file_size = len(data)
        if offset < file_size:
            if offset + size > file_size:
                size = file_size - offset
            return data[offset:offset+size]
        else:
            return ''

    def unlink(self, path):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"unlink: path %s" % path)

        info = self.getinfo(path)
        self.api.remove_bulk_by_id([info.object_id])
        head, tail = os.path.split(path)
        self.clear_filecache(path)
        if path in self.infocache:
            self.infocache.pop(path)
        if head in self.dircache:
            self.dircache.pop(head)

    def rmdir(self, path):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"rmdir: path %s" % path)
        self.unlink(path)

    def mkdir(self, path, mode):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"mkdir: path %s, mode %s" % (path, mode))

        head, tail = os.path.split(path)
        newdir = self.api.create_by_path(head.encode('utf_8'), tail.encode('utf_8'), Type=pyacd.types.FOLDER)
        if path in self.infocache:
            self.infocache.pop(path)
        if head in self.dircache:
            self.dircache.pop(head)

    def rename(self, srcpath, dstpath):
        srcpath = srcpath.decode('utf_8')
        dstpath = dstpath.decode('utf_8')
        self.log.debug(u"rename: srcpath %s, dstpath %s" % (srcpath, dstpath))

        srcinfo = self.getinfo(srcpath)
        dsthead, dsttail = os.path.split(dstpath)
        dstinfo = self.getinfo(dsthead)

        self.api.move_by_id(srcinfo.object_id, dstinfo.object_id, dsttail.encode('utf_8'), overwrite=True)

        if srcpath in self.infocache:
            self.infocache.pop(srcpath)
        if dsthead in self.dircache:
            self.dircache.pop(dsthead)

    def chmod(self, path, mode):
        return 0
    def chown(self, path, uid, gid):
        return 0
    def utime(self, path, times):
        return 0
    def truncate(self, path, size):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"truncate: path %s, size %d" % (path, size))
        head, tail = os.path.split(path)

        info = self.getinfo(path)
        if size == 0:
            print u"clearing filecache", path
            self.clear_filecache(path)

        return 0

    def create(self, path, flags, mode):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"create: path %s, flags %s, mode %s" % (path, flags, mode))

        head, tail = os.path.split(path)
        self.infocache[path] = self.api.create_by_path(head.encode('utf_8'), tail.encode('utf_8'))

    def write(self, path, buf, offset):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"write: path %s, offset %d" % (path, offset))

        self.log.debug(u"Starting write of %s" % path)
        self.write_filecache(path, buf)
        return len(buf)

    def flush(self, path):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"flush: path %s" % path)

        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        try:
            info = self.getinfo(path)
            if self.check_filecache(path):
                # we're all good; the remote size and MD5 match the filecache's
                self.log.debug(u"filecache matches; we're done here")
                return 0
            else:
                self.log.debug(u"flush: reading filecache for %s" % path)
                buf = self.read_filecache(path)
                blen = len(buf)
                url = self.api.get_upload_url_by_id(info.object_id, blen)
                result = self.api.upload(url.http_request.end_point, url.http_request.parameters, basename.encode('utf_8'), buf)
                result = self.api.complete_file_upload_by_id(url.object_id, url.storage_key)
                if dirname in self.dircache:
                    self.dircache.pop(dirname)
                if path in self.infocache:
                    self.infocache.pop(path)
                return 0

        except pyacd.exception.PyAmazonCloudDriveError:
            # file does not exist; this should never happen since create() creates it
            pass

        return -ENOENT

    def release(self, path, flags):
        path = isinstance(path, unicode) and path or path.decode('utf_8')
        self.log.debug(u"release: path %s, flags %s" % (path, flags))

        self.gc_filecache()

        try:
            info = self.getinfo(path)
            if not self.check_filecache(path):
                self.log.debug(u"flush: reading filecache for %s" % path)
                buf = self.read_filecache(path)
                return len(buf)
            else:
                return len(self.read_filecache(path))
        except pyacd.exception.PyAmazonCloudDriveError:
             # file does not exist; this should never happen since create() creates it
            pass

        return 0

if __name__ == '__main__':
    if os.geteuid() == 0:
        print "Do not run me as root or with sudo!"
        exit(1)

    fs = ACDFS()
    fs.flags = 0
    fs.multithreaded = 0

    fs.fuse_args.add('big_writes')
    fs.fuse_args.add('direct_io')

    fs.parser.add_option(mountopt="email", metavar="EMAIL",
                         help="EMAIL login for Amazon Cloud Drive")
    fs.parser.add_option(mountopt="password", metavar="PASSWORD",
                         help="PASSWORD password for Amazon Cloud Drive")
    fs.parser.add_option(mountopt="cachefree",
                         help="What percentage of the filecache drive we must keep free (defaults to 10%)")

    fs.parse(values=fs, errex=1)

    fs.main()
